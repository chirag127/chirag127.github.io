from .schema import UnifiedModel

TIER_6_MODELS: list[UnifiedModel] = [
    UnifiedModel(
        name="Mixtral 8x7B Instruct Groq",
        size_billions=56.0,
        description="Mixtral 8x7B MoE - Excellent Code",
        max_tokens=32768,
        working=True,
        providers=[("groq", "mistralai/mixtral-8x7b-instruct-v0.1")],
    ),
    UnifiedModel(
        name="Mixtral 8x7B Instruct OpenRouter",
        size_billions=56.0,
        description="Mixtral 8x7B MoE - Excellent Code",
        max_tokens=32768,
        working=True,
        providers=[("openrouter", "mistralai/mixtral-8x7b-instruct:free")],
    ),
    UnifiedModel(
        name="Phi 4 GitHub",
        size_billions=40.0,
        description="Microsoft Phi-4 - Strong Reasoning",
        max_tokens=128000,
        working=True,
        providers=[("github", "Phi-4")],
    ),
    UnifiedModel(
        name="Phi 4 OpenRouter",
        size_billions=40.0,
        description="Microsoft Phi-4 - Strong Reasoning",
        max_tokens=128000,
        working=True,
        providers=[("openrouter", "microsoft/phi-4:free")],
    ),
    UnifiedModel(
        name="Qwen QwQ 32B Groq",
        size_billions=32.0,
        description="Qwen QwQ 32B - Reasoning Focused",
        max_tokens=32768,
        working=True,
        providers=[("groq", "qwen-qwq-32b")],
    ),
    UnifiedModel(
        name="Qwen QwQ 32B Cloudflare",
        size_billions=32.0,
        description="Qwen QwQ 32B - Reasoning Focused",
        max_tokens=32768,
        working=False, # Auth
        providers=[("cloudflare", "@cf/qwen/qwq-32b")],
    ),
    UnifiedModel(
        name="Qwen QwQ 32B OpenRouter",
        size_billions=32.0,
        description="Qwen QwQ 32B - Reasoning Focused",
        max_tokens=32768,
        working=True,
        providers=[("openrouter", "qwen/qwq-32b:free")],
    ),
    UnifiedModel(
        name="Qwen3 32B Instruct Groq",
        size_billions=32.0,
        description="Qwen 3 32B - Solid Multilingual",
        max_tokens=32768,
        working=True,
        providers=[("groq", "qwen/qwen3-32b")],
    ),
    UnifiedModel(
        name="Qwen3 32B Instruct Cerebras",
        size_billions=32.0,
        description="Qwen 3 32B - Solid Multilingual",
        max_tokens=32768,
        working=True,
        providers=[("cerebras", "Qwen-3-32B")],
    ),
    UnifiedModel(
        name="Qwen3 32B Instruct OpenRouter",
        size_billions=32.0,
        description="Qwen 3 32B - Solid Multilingual",
        max_tokens=32768,
        working=True,
        providers=[("openrouter", "qwen/qwen3-32b:free")],
    ),
    UnifiedModel(
        name="Qwen 2.5 Coder 32B Instruct Cloudflare",
        size_billions=32.0,
        description="Qwen 2.5 Coder 32B - Code Specialized",
        max_tokens=32768,
        working=False, # Auth
        providers=[("cloudflare", "@cf/qwen/qwen2.5-coder-32b-instruct")],
    ),
    UnifiedModel(
        name="Qwen 2.5 Coder 32B Instruct OpenRouter",
        size_billions=32.0,
        description="Qwen 2.5 Coder 32B - Code Specialized",
        max_tokens=32768,
        working=True,
        providers=[("openrouter", "qwen/qwen2.5-coder-32b-instruct:free")],
    ),
    UnifiedModel(
        name="Nemotron 3 Nano 30B Nvidia",
        size_billions=30.0,
        description="NVIDIA Nemotron 3 30B",
        max_tokens=8192,
        working=True,
        providers=[("nvidia", "nvidia/nemotron-3-nano-30b-a3b")],
    ),
    UnifiedModel(
        name="Nemotron 3 Nano 30B OpenRouter",
        size_billions=30.0,
        description="NVIDIA Nemotron 3 30B",
        max_tokens=8192,
        working=True,
        providers=[("openrouter", "nvidia/nemotron-3-nano-30b-a3b:free")],
    ),
    UnifiedModel(
        name="Gemma 3 27B Instruct Gemini",
        size_billions=27.0,
        description="Gemma 3 27B - Strong Google Model",
        max_tokens=8192,
        working=True,
        providers=[("gemini", "gemma-3-27b-it")],
    ),
    UnifiedModel(
        name="Gemma 3 27B Instruct Cloudflare",
        size_billions=27.0,
        description="Gemma 3 27B - Strong Google Model",
        max_tokens=8192,
        working=False, # Auth
        providers=[("cloudflare", "@cf/google/gemma-3-27b-instruct")],
    ),
    UnifiedModel(
        name="Gemma 3 27B Instruct OpenRouter",
        size_billions=27.0,
        description="Gemma 3 27B - Strong Google Model",
        max_tokens=8192,
        working=True,
        providers=[("openrouter", "google/gemma-3-27b-instruct:free")],
    ),
    UnifiedModel(
        name="Mistral Small 3.1 24B Instruct Cloudflare",
        size_billions=24.0,
        description="Mistral Small 3.1 24B - All-rounder",
        max_tokens=32768,
        working=False, # Auth
        providers=[("cloudflare", "@cf/mistral/mistral-small-3.1-24b-instruct")],
    ),
    UnifiedModel(
        name="Mistral Small 3.1 24B Instruct GitHub",
        size_billions=24.0,
        description="Mistral Small 3.1 24B - All-rounder",
        max_tokens=32768,
        working=True,
        providers=[("github", "Mistral-Small-3.1")],
    ),
    UnifiedModel(
        name="Mistral Small 3.1 24B Instruct OpenRouter",
        size_billions=24.0,
        description="Mistral Small 3.1 24B - All-rounder",
        max_tokens=32768,
        working=True,
        providers=[("openrouter", "mistral/mistral-small-3.1-24b-instruct:free")],
    ),
    UnifiedModel(
        name="Codestral 25.01 Mistral",
        size_billions=22.0,
        description="Codestral - Code Specialized",
        max_tokens=8192,
        working=True,
        providers=[("mistral", "codestral-latest")],
    ),
    UnifiedModel(
        name="Codestral 25.01 GitHub",
        size_billions=22.0,
        description="Codestral - Code Specialized",
        max_tokens=8192,
        working=True,
        providers=[("github", "Codestral-25.01")],
    ),
    UnifiedModel(
        name="Codestral 25.01 OpenRouter",
        size_billions=22.0,
        description="Codestral - Code Specialized",
        max_tokens=8192,
        working=True,
        providers=[("openrouter", "mistral/codestral-25.01:free")],
    ),
    UnifiedModel(
        name="GPT OSS 20B OpenRouter",
        size_billions=20.0,
        description="GPT-OSS 20B - Fallback fallback",
        max_tokens=131072,
        working=False, # Likely not available
        providers=[("openrouter", "openai/gpt-oss-20b:free")],
    ),
    UnifiedModel(
        name="GPT OSS 20B Cerebras",
        size_billions=20.0,
        description="GPT-OSS 20B - Fallback fallback",
        max_tokens=131072,
        working=True, # Verified working often
        providers=[("cerebras", "gpt-oss-120b")], # Should be gpt-oss-20b or generic
    ),
    UnifiedModel(
        name="GPT OSS 20B Cloudflare",
        size_billions=20.0,
        description="GPT-OSS 20B - Fallback fallback",
        max_tokens=131072,
        working=False, # Auth
        providers=[("cloudflare", "@cf/openai/gpt-oss-20b")],
    ),
]
